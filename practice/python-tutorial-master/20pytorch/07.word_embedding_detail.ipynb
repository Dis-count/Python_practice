{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二课 词向量\n",
    "\n",
    "第二课学习目标\n",
    "- 学习词向量的概念\n",
    "- 用Skip-thought模型训练词向量\n",
    "- 学习使用PyTorch dataset和dataloader\n",
    "- 学习定义PyTorch模型\n",
    "- 学习torch.nn中常见的Module\n",
    "    - Embedding\n",
    "- 学习常见的PyTorch operations\n",
    "    - bmm\n",
    "    - logsigmoid\n",
    "- 保存和读取PyTorch模型\n",
    "    \n",
    "\n",
    "第二课使用的训练数据可以从以下链接下载到。\n",
    "\n",
    "链接:https://pan.baidu.com/s/1tFeK3mXuVXEy3EMarfeWvg  密码:v2z5\n",
    "\n",
    "在这一份notebook中，我们会（尽可能）尝试复现论文[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)中训练词向量的方法. 我们会实现Skip-gram模型，并且使用论文中noice contrastive sampling的目标函数。\n",
    "\n",
    "这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。\n",
    "\n",
    "以下是一些我们没有实现的细节\n",
    "- subsampling：参考论文section 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  #神经网络工具箱torch.nn \n",
    "import torch.nn.functional as F  #神经网络函数torch.nn.functional\n",
    "import torch.utils.data as tud  #Pytorch读取训练集需要用到torch.utils.data类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**两个模块的区别：**[torch.nn 和 torch.functional 的区别](https://blog.csdn.net/hawkcici160/article/details/80140059)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter  #参数更新和优化函数\n",
    "\n",
    "from collections import Counter #Counter 计数器\n",
    "import numpy as np \n",
    "import random\n",
    "import math \n",
    "\n",
    "import pandas as pd\n",
    "import scipy #SciPy是基于NumPy开发的高级模块，它提供了许多数学算法和函数的实现\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity #余弦相似度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available() #有GPU可以用\n",
    "\n",
    "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
    "random.seed(53113)\n",
    "np.random.seed(53113)\n",
    "torch.manual_seed(53113)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(53113)\n",
    "    \n",
    "# 设定一些超参数   \n",
    "K = 100 # number of negative samples 负样本随机采样数量\n",
    "C = 3 # nearby words threshold 指定周围三个单词进行预测\n",
    "NUM_EPOCHS = 2 # The number of epochs of training 迭代轮数\n",
    "MAX_VOCAB_SIZE = 10000 # the vocabulary size 词汇表多大\n",
    "BATCH_SIZE = 32 # the batch size 每轮迭代1个batch的数量\n",
    "LEARNING_RATE = 0.2 # the initial learning rate #学习率\n",
    "EMBEDDING_SIZE = 100 #词向量维度\n",
    "       \n",
    "    \n",
    "LOG_FILE = \"word-embedding.log\"\n",
    "\n",
    "# tokenize函数，把一篇文本转化成一个个单词\n",
    "def word_tokenize(text): \n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从文本文件中读取所有的文字，通过这些文本创建一个vocabulary\n",
    "- 由于单词数量可能太大，我们只选取最常见的MAX_VOCAB_SIZE个单词\n",
    "- 我们添加一个UNK单词表示所有不常见的单词\n",
    "- 我们需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/nietzsche.txt\", \"r\") as fin: #读入文件\n",
    "    text = fin.read()\n",
    "    \n",
    "text = [w for w in word_tokenize(text.lower())] \n",
    "#分词，在这里类似于text.split()\n",
    "\n",
    "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE-1))\n",
    "#字典格式，把（MAX_VOCAB_SIZE-1）个最频繁出现的单词取出来，-1是留给不常见的单词\n",
    "\n",
    "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
    "#unk表示不常见单词数=总单词数-常见单词数\n",
    "#这里计算的到vocab[\"<unk>\"]=29999\n",
    "\n",
    "idx_to_word = [word for word in vocab.keys()] \n",
    "#取出字典的所有单词key\n",
    "\n",
    "word_to_idx = {word:i for i, word in enumerate(idx_to_word)}\n",
    "#取出所有单词的单词和对应的索引，索引值与单词出现次数相反，最常见单词索引为0。\n",
    "\n",
    "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
    "#所有单词的频数values\n",
    "\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "#所有单词的频率\n",
    "\n",
    "word_freqs = word_freqs ** (3./4.)\n",
    "#论文里乘以3/4次方\n",
    "\n",
    "word_freqs = word_freqs / np.sum(word_freqs) # 用来做 negative sampling\n",
    "# 重新计算所有单词的频率\n",
    "\n",
    "VOCAB_SIZE = len(idx_to_word) #词汇表单词数30000=MAX_VOCAB_SIZE\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现Dataloader\n",
    "\n",
    "一个dataloader需要以下内容：\n",
    "\n",
    "- 把所有text编码成数字，然后用subsampling预处理这些文字。\n",
    "- 保存vocabulary，单词count，normalized word frequency\n",
    "- 每个iteration sample一个中心词\n",
    "- 根据当前的中心词返回context单词\n",
    "- 根据中心词sample一些negative单词\n",
    "- 返回单词的counts\n",
    "\n",
    "这里有一个好的tutorial介绍如何使用[PyTorch dataloader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "为了使用dataloader，我们需要定义以下两个function:\n",
    "\n",
    "- ```__len__``` function需要返回整个数据集中有多少个item\n",
    "- ```__get__``` 根据给定的index返回一个item\n",
    "\n",
    "有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(tud.Dataset): #tud.Dataset父类\n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):\n",
    "        ''' text: a list of words, all text from the training dataset\n",
    "            word_to_idx: the dictionary from word to idx\n",
    "            idx_to_word: idx to word mapping\n",
    "            word_freq: the frequency of each word\n",
    "            word_counts: the word counts\n",
    "        '''\n",
    "        super(WordEmbeddingDataset, self).__init__() #初始化模型\n",
    "        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE-1) for t in text]\n",
    "        #字典 get() 函数返回指定键的值（第一个参数），如果值不在字典中返回默认值（第二个参数）。\n",
    "        #取出text里每个单词word_to_idx字典里对应的索引,不在字典里返回\"<unk>\"的索引\n",
    "        #\"<unk>\"的索引=29999，get括号里第二个参数应该写word_to_idx[\"<unk>\"]，不应该写VOCAB_SIZE-1，虽然数值一样。\n",
    "        \n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "        #变成tensor类型，这里变成longtensor，也可以torch.LongTensor(self.text_encoded)\n",
    "        \n",
    "        self.word_to_idx = word_to_idx #保存数据\n",
    "        self.idx_to_word = idx_to_word  #保存数据\n",
    "        self.word_freqs = torch.Tensor(word_freqs) #保存数据\n",
    "        self.word_counts = torch.Tensor(word_counts) #保存数据\n",
    "        \n",
    "    def __len__(self): #数据集有多少个item \n",
    "        #魔法函数__len__\n",
    "        ''' 返回整个数据集（所有单词）的长度\n",
    "        '''\n",
    "        return len(self.text_encoded) #所有单词的总数\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #魔法函数__getitem__，这个函数跟普通函数不一样\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的(positive)单词\n",
    "            - 随机采样的K个单词作为negative sample\n",
    "        '''\n",
    "        center_word = self.text_encoded[idx] \n",
    "        #print(center_word)\n",
    "        #中心词索引\n",
    "        #这里__getitem__函数是个迭代器，idx代表了所有的单词索引。\n",
    "        \n",
    "        pos_indices = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1))\n",
    "        #周围词索引的索引，比如idx=0时。pos_indices = [-3, -2, -1, 1, 2, 3] \n",
    "        \n",
    "        pos_indices = [i%len(self.text_encoded) for i in pos_indices]\n",
    "        #range(idx+1, idx+C+1)超出词汇总数时，需要特别处理，取余数\n",
    "        \n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        #周围词索引，就是希望出现的正例单词\n",
    "        #print(pos_words)\n",
    "        \n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\n",
    "        #负例采样单词索引，torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标。\n",
    "        #取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大。\n",
    "        #每个正确的单词采样K个，pos_words.shape[0]是正确单词数量\n",
    "        #print(neg_words)\n",
    "        \n",
    "        return center_word, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建dataset和dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
    "# list(dataset) 可以把尝试打印下center_word, pos_words, neg_words看看\n",
    "\n",
    "dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.utils.data.DataLoader理解：https://blog.csdn.net/qq_36653505/article/details/83351808"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义PyTorch模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        ''' 初始化输出和输出embedding\n",
    "        '''\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.vocab_size = vocab_size  #30000\n",
    "        self.embed_size = embed_size  #100\n",
    "        \n",
    "        initrange = 0.5 / self.embed_size\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        #模型输出nn.Embedding(30000, 100)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        #权重初始化的一种方法\n",
    "        \n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "         #模型输入nn.Embedding(30000, 100)\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        #权重初始化的一种方法\n",
    "        \n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        '''\n",
    "        input_labels: 中心词, [batch_size]\n",
    "        pos_labels: 中心词周围 context window 出现过的单词 [batch_size * (window_size * 2)]\n",
    "        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (window_size * 2 * K)]\n",
    "        \n",
    "        return: loss, [batch_size]\n",
    "        '''\n",
    "        \n",
    "        batch_size = input_labels.size(0)  #input_labels是输入的标签，tud.DataLoader()返回的。相已经被分成batch了。\n",
    "        \n",
    "        input_embedding = self.in_embed(input_labels) \n",
    "        # B * embed_size\n",
    "        #这里估计进行了运算：（128,30000）*（30000,100）= 128(B) * 100 (embed_size)\n",
    "        \n",
    "        pos_embedding = self.out_embed(pos_labels) # B * (2*C) * embed_size\n",
    "        #同上，增加了维度(2*C)，表示一个batch有B组周围词单词，一组周围词有(2*C)个单词，每个单词有embed_size个维度。\n",
    "        \n",
    "        neg_embedding = self.out_embed(neg_labels) # B * (2*C * K) * embed_size\n",
    "        #同上，增加了维度(2*C*K)\n",
    "      \n",
    "    \n",
    "        #torch.bmm()为batch间的矩阵相乘（b,n.m)*(b,m,p)=(b,n,p)\n",
    "        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)).squeeze() # B * (2*C)\n",
    "        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze() # B * (2*C*K)\n",
    "        #unsqueeze(2)指定位置升维，.squeeze()压缩维度。\n",
    "        \n",
    "        #下面loss计算就是论文里的公式\n",
    "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1) # batch_size     \n",
    "        loss = log_pos + log_neg\n",
    "        \n",
    "        return -loss\n",
    "    \n",
    "    def input_embeddings(self):   #取出self.in_embed数据参数\n",
    "        return self.in_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个模型以及把模型移动到GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "#得到model，有参数，有loss，可以优化了\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "下面是评估模型的代码，以及训练模型的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def evaluate(filename, embedding_weights): \n",
    "    if not os.path.isfile(filename):\n",
    "        return \n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename, sep=\",\")\n",
    "    else:\n",
    "        data = pd.read_csv(filename, sep=\"\\t\")\n",
    "    human_similarity = []\n",
    "    model_similarity = []\n",
    "    for i in data.iloc[:, 0:2].index:\n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
    "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
    "            continue\n",
    "        else:\n",
    "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
    "            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
    "            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))\n",
    "            human_similarity.append(float(data.iloc[i, 2]))\n",
    "\n",
    "    return scipy.stats.spearmanr(human_similarity, model_similarity)# , model_similarity\n",
    "\n",
    "def find_nearest(word):\n",
    "    index = word_to_idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    return [idx_to_word[i] for i in cos_dis.argsort()[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型：\n",
    "- 模型一般需要训练若干个epoch\n",
    "- 每个epoch我们都把所有的数据分成若干个batch\n",
    "- 把每个batch的输入和输出都包装成cuda tensor\n",
    "- forward pass，通过输入的句子预测每个单词的下一个单词\n",
    "- 用模型的预测和正确的下一个单词计算cross entropy loss\n",
    "- 清空模型当前gradient\n",
    "- backward pass\n",
    "- 更新模型参数\n",
    "- 每隔一定的iteration输出模型在当前iteration的loss，以及在验证数据集上做模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9999,  160,    2,   10, 1733,   77,   20,   89,   70, 6584,   64,    0,\n          79, 4682, 9999, 9999, 2160, 9999,  352, 1623,    1,   98, 4701,   87,\n           2,  107, 1550,  900,    3,   21,  254,  169]) tensor([[3190, 2821,    6, 9999,   14, 9999],\n        [1983,   48,  314, 4398,    2, 1946],\n        [  38,  131,  330,    3,  124, 4494],\n        [ 185,   94, 9999, 9999,  230,   78],\n        [3023, 9999,    6,   56,    0, 4897],\n        [9999, 1217,  138, 9999,  600,    3],\n        [  72,   40,    9,   75,   51, 7299],\n        [ 133,    4,    0, 1398, 2005,    2],\n        [ 157,    7,    9, 5278,    4,  860],\n        [1426,    0, 3836,    0, 1442, 6585],\n        [9999, 3369, 5145,    0, 9999, 9999],\n        [   0, 9999,    1,   87, 9999, 9999],\n        [   1,  648,   27,  466,   18,  769],\n        [  25, 9509,    0, 1405,    2, 2256],\n        [2501,    1, 2482,   65,   40,   48],\n        [ 729,    1, 4753, 2861,    2, 4943],\n        [   0,  270,    1,  118,  914,   78],\n        [ 387,    2,   25,   35,   54, 1519],\n        [  71,  935,   26,   10,   19,  270],\n        [  21, 9999,    2,    0, 4151,    0],\n        [2309, 2310,  159, 4289,    1,  233],\n        [2428,    1,   79,   53,    0,  510],\n        [   0, 9589,    1,   11,   48,   23],\n        [9999,   41,    0, 9999,    1, 9999],\n        [   1,  191, 1312,  191, 9999,   52],\n        [  12, 1439,   94, 1353,    2,   38],\n        [7542,   13,    3,    3,   44,   71],\n        [ 900,   83,  365, 7598,  962, 7599],\n        [   0,  551, 2928,  182,   19, 9999],\n        [   6, 2157,    3,   54,   13,   24],\n        [   2, 4834,    4, 3080,   37,   13],\n        [ 177,   11,  604,    4,  429,   26]]) tensor([[   4,  845,   21,  ..., 3605, 9737, 6046],\n        [ 135, 7068, 1633,  ..., 9593,   13,  577],\n        [1938,  615,  623,  ...,  900,  381, 7445],\n        ...,\n        [9548,   13, 9999,  ..., 8056,  171, 2251],\n        [   0, 4767,   16,  ...,    2,   27, 4991],\n        [ 272,    1,  791,  ...,   29,   18, 9627]])\ntensor([ 396,  111,    3,  139,  345,    5, 7638,  849, 2433, 9999,  332,    2,\n          39,   27,   63, 2238,    0,    1,  155, 2402, 9999,  348,    3, 9999,\n         486,    3,    3,   30,    6,  253,   28,  205]) tensor([[1326,    1,  880,   47,   13,    0],\n        [  24,  834,  434,   80,    2, 2458],\n        [   7, 2063,  568,   77, 3622,   40],\n        [  22, 9999, 9999, 3159, 9999,    0],\n        [  37,   12, 9999,   64, 1862,   14],\n        [ 376,  578,    7, 9999,    1, 9999],\n        [7635, 7636, 7637,    2,  211,   79],\n        [ 849,    1,  158,    8,   53,   28],\n        [   1,    5,   87,   26, 9999,    2],\n        [  50,  206,  100,    0,   89,  672],\n        [  40,  605,    1,    6, 9999, 4941],\n        [ 105,   87,  889, 8925,    0,  281],\n        [2025,    1, 2587,   52,   23,  918],\n        [2774,    2, 6706,    0, 1448,  621],\n        [2460,    4,   44, 9999,   26, 2895],\n        [1076, 7860, 7861, 7862, 7863, 1308],\n        [   5,  811,    1, 5244,   37,    3],\n        [   0, 2239,  780,    0,   91,  402],\n        [2400,   29,    5, 8954,  129, 1915],\n        [ 641,   78, 1662,   10,    0, 3449],\n        [ 726,   57, 9999,    6,   45,  390],\n        [  97,   13,   23, 1904,    9,   47],\n        [  53,   21,  363,  528,   24,  297],\n        [2479,    2, 2344,  830,    4,  188],\n        [  12, 2157,    3,    7,    0, 4579],\n        [   9,    6,  996,  241,    0,  187],\n        [  17,   12, 2157,  486,    7,    0],\n        [3423,    2,  133,   78,  163,  231],\n        [   0, 9999,  681,   33,   45,  104],\n        [9999,   31, 1195,   40, 5379, 9999],\n        [1012, 9999,  204,   81,   65, 9999],\n        [   6,   65,    0, 1394,    1,  844]]) tensor([[   5,  139,    1,  ...,    1, 1449, 9999],\n        [3874,   20, 1675,  ..., 4592, 1905, 9506],\n        [9356, 2523,   17,  ...,    0, 8092,  595],\n        ...,\n        [2200, 3914, 5229,  ...,    5, 9055,    2],\n        [   4,  998, 4933,  ..., 5769,  436,   25],\n        [ 131,  173, 4351,  ..., 4208,   30, 9414]])\ntensor([   2,   10,  394, 2194,    0,    0,    1,  287,  179,   32,    4,  113,\n           1,   12,  464, 9999,   63, 5569,    0, 4492,    9,    6,    9,    3,\n          99,   13,  396,  146, 3974,    1, 9999, 1740]) tensor([[4845, 9999, 9999, 9999,   60,   85],\n        [3555, 9999, 9999, 1546,    2, 1546],\n        [   4,    0,  946,    1,  131,  702],\n        [  87,  198,   20,  345,    3,    5],\n        [9999, 3144,    2,  886,    1,   96],\n        [  16, 1461,    1,  173, 1280,    1],\n        [   4,    0, 1585,    0,  543,   27],\n        [ 323, 9999,    6, 5728,    4,   17],\n        [   7,  235,   37,   12,  161,   29],\n        [  19,  872,    8, 9999, 1777,  301],\n        [9999,    2, 9999,   17,   25, 1249],\n        [ 208,   67, 2896,  459,    2, 9999],\n        [  33,    0, 3325,  271,    1, 9999],\n        [   1, 9999,    3, 2953,    3,  274],\n        [ 353,    2,   14, 9966,    4,    0],\n        [ 840,    1,  107,  138,    0, 1220],\n        [4272,    6,    5, 1334,  159,    1],\n        [  58, 9999,    0,    1,    5, 9999],\n        [3350, 4249, 9999, 2253,    1, 9999],\n        [   5,   39,   52,   24,  834,  434],\n        [  10, 9999, 1160,  548, 1523,   66],\n        [3107,    7,  885, 1555,    3,   72],\n        [   0, 1517,    1,   17,    6,  467],\n        [  31, 3346,  186, 9999,  707,    4],\n        [1591,    1,   44,   18,   24,    0],\n        [   7,  112,   18, 6562,   27,  230],\n        [   0, 4077,    1,    4,    5,  116],\n        [   3,  948,   16,    8,   73, 9999],\n        [   2, 2197,   16,   48,  314,  871],\n        [   3,    0,  135,    0,  409,    8],\n        [ 100,   16, 9999,    0,  344,    1],\n        [ 402,    3,   12,    2, 9999,   36]]) tensor([[2964,  807,    5,  ...,  124, 1836, 2260],\n        [5278,   35,   92,  ..., 8307, 5220,   15],\n        [3951, 1811, 2874,  ..., 6242,   46, 5375],\n        ...,\n        [1110, 3323,   53,  ...,   42,  255, 1270],\n        [ 502, 3424,  658,  ...,    4, 1482, 3400],\n        [   3, 9999, 1589,  ..., 4901,   52,    2]])\ntensor([   5,    5, 1201,  332,   35,   80, 5902,   84,  180,   20,   10,   10,\n        9999,  243,  865,  534,  365, 2557,  276,   25, 9999,   45,    4,  880,\n          26,    2,    1,    0, 8696,   33,  164,   69]) tensor([[ 208,   67,  233,   90, 9999,  843],\n        [   3, 1342,   43,  433, 1310,  138],\n        [2616,  607,   14,    2, 5917, 5918],\n        [  15, 5800,   16,  538,  218, 5801],\n        [  18,  646, 9999,   18,  209,  973],\n        [  12, 1936,   27,   78,  255,  839],\n        [   1,   22, 5454,  308,  100,    0],\n        [ 135,    1, 9999,   15,   51,  916],\n        [  73, 7353,    4, 3966,    2,    4],\n        [2039,    7,  252,    8,   85,  421],\n        [4393,  417,   13,    0,  269,    7],\n        [  20,  609, 5931,    0,  383,    1],\n        [ 250,   12,  567,    3, 1918,   75],\n        [   4,  656,  283,  522,  390,    2],\n        [  20,  176,  393,   66,  511,   69],\n        [   2,   46, 9999,    7,  179,   84],\n        [  44, 1479,   44,  330,    3,    0],\n        [ 200,   81,   21,    2, 5754,    8],\n        [ 241,    7,    0, 3384,  127,   33],\n        [ 130,    8,    1, 9999,    3, 4157],\n        [ 276, 1175,    1,    6, 9999, 5692],\n        [1397,    7,   20,  281,    3,   12],\n        [2058,    2, 2176,    5,   70,  370],\n        [1562,    1,   22,  612,    1,  441],\n        [ 475,  213, 2343, 9311,    9,    6],\n        [  39,  511,   69,  924,    0,  415],\n        [9999, 9999, 9999, 5605, 2585,    1],\n        [   1,  428, 1113, 6899, 6900,   41],\n        [8695, 2851,    3,   15,  202,   13],\n        [ 516,  333, 4053, 7189, 7190,  262],\n        [2200,   40,    6, 7015,    2,    0],\n        [  17, 8567,   17,  488,   17,  667]]) tensor([[1517,  123,   26,  ..., 2038,  187, 2353],\n        [ 163,    0, 1212,  ...,  423, 1282, 2139],\n        [1276, 2102, 8626,  ..., 8893,  368, 1397],\n        ...,\n        [   0, 2099, 9060,  ..., 7646,   24,  476],\n        [   0, 7871,   72,  ..., 4905, 6594, 3181],\n        [   1,  108,    0,  ...,  591, 2929,    0]])\ntensor([ 162, 1045,   21, 9999, 4722,   10,   10,    0,   19,    3,   14,    5,\n           8,  542, 3670, 1483,   77,  908,    2,  229, 2105, 1770,  209, 2206,\n         339, 9999,  187,   16,   36,  385, 8470,   24]) tensor([[9999,    5,  148,  853, 5584,  239],\n        [   2,    7,   97,   12,  552,   77],\n        [9999,  107, 9999, 3778,    7,   19],\n        [  12,   13, 9999, 9999,  159, 2037],\n        [  12, 1827,    0,    7,    9,   37],\n        [  46, 3059, 2365, 3060,   38,    7],\n        [ 650,   10, 2574,  418, 2557,    2],\n        [   2,    0, 9999, 1408,  947,    4],\n        [   0, 1948,    1, 9714,   21,   20],\n        [1228,  248, 6175,    7,  542,   83],\n        [  29, 8326,  181,   25, 2272,  136],\n        [  12,  342,    8,  773, 9999,  185],\n        [9999, 9999, 9999, 9999,  643,  122],\n        [  12,    0, 1348,    2,   13,    5],\n        [   7,    0, 2798,    1, 1111,   11],\n        [9999,  358,   46, 9999,   33,   11],\n        [2629,    7,  208, 2516,  114,    5],\n        [ 478,   10,    0,    1,    0, 5258],\n        [2250, 9999, 4824, 4825, 9999,  423],\n        [   2, 9999,    1,  471,    2, 2371],\n        [  83,  937, 9999, 9999, 2105, 9999],\n        [  56,  204,   28,   26,  709,   19],\n        [ 105,  323, 2905, 1180,    4,    0],\n        [  40,  270,    6,   33,    0, 2119],\n        [   3,  310,    3,    0, 1669,  309],\n        [  90, 9999,    2,    3, 9999,    0],\n        [9999,   17,   44, 1773,   44, 9999],\n        [   0, 7909,    1, 7910,    2, 2971],\n        [  63, 9999, 2012,    7,   50,  152],\n        [1924,   39,   93,   61, 9999,    0],\n        [ 842,   27,    0,    1,   25,  374],\n        [  31,   20,  497,   21,   95,   26]]) tensor([[  27,  307, 3005,  ...,  648, 2554, 3211],\n        [1383, 1704,  396,  ..., 9259,   56,   38],\n        [ 242, 5000, 1470,  ...,   96,   17,   17],\n        ...,\n        [1494,   46, 1283,  ..., 2640,   31, 4441],\n        [  73,   43,  989,  ..., 2443, 2159,   92],\n        [ 471, 3199,    0,  ...,    3,  458,  993]])\ntensor([ 886, 7521,  245,  225, 1629, 9999,   10, 1737,  583,    1,   18, 9802,\n           1,    0, 9999,    3,   62,    0,   22,    6,   14,  251, 1185,   18,\n         667, 9999,    2,   34,    6,    4, 9999,   19]) tensor([[   6,    0,  647,    1, 1172,   34],\n        [4148,    1,   16,    2,  705,   15],\n        [ 147, 1271,    5,    1,  501,   53],\n        [9844,    0,   46,    5,   39,   83],\n        [  91,    2,    0,  389,  183,    4],\n        [1659, 1595,    1,    4,  968,   14],\n        [  92, 7942,  233,    0,  115, 2309],\n        [ 271,   56,   74, 1179,    0, 9999],\n        [9301,    6,    0,    1,   22,  418],\n        [4401,   17,  120,    0, 2351,    2],\n        [ 126,   11, 3372, 4621,    0,  378],\n        [  78,   13,   12,   15,   78, 1031],\n        [   3,    0,  588,    5, 2424,   31],\n        [9999,    0, 9999, 9999,    2,   92],\n        [9999, 9999,    7,    6,  207,  515],\n        [ 130,  303, 6162,  109,  611, 6163],\n        [ 157,    7,   15,   13,  215,  282],\n        [ 759,    6,  101,  313,   11,   20],\n        [ 851, 1382,   33, 3187,    1,   44],\n        [  36,  183,  125, 1883,    0,  175],\n        [   0, 5394, 9999,   16, 3300, 1695],\n        [  20, 3028,  139,   10, 8298,  752],\n        [  48, 3103,    7,  468,    3,    0],\n        [   1,  322,   31,   46,  379,    3],\n        [1914,    6,   45,    7,   35,    6],\n        [1394,    3,  740,  200,    8,    0],\n        [  14, 1701, 2100, 1260,   67,  909],\n        [1788,  128,   18,   26,  227,  861],\n        [  27, 3972, 6993,   34, 1281,   59],\n        [2030,   76, 9999,   50, 5343,  715],\n        [  83, 9999,   15,    4,    5,  956],\n        [   1,    0, 5352,  621,    1,    0]]) tensor([[  27,  228,  358,  ..., 7434, 1053, 7724],\n        [ 248, 2984, 5546,  ...,  227, 1704,  240],\n        [ 364,   18, 1403,  ..., 1411, 3057,  573],\n        ...,\n        [2857, 5620, 2238,  ..., 7997, 2358, 9770],\n        [ 138, 3577, 3615,  ..., 1710, 1743, 1647],\n        [3710,   52,  223,  ...,  816, 9695, 6426]])\ntensor([9999, 5499, 9999, 3530,  289,    0,   42,    3, 9999,  737,    0,    3,\n        2566,    9, 3420,    0,   77, 3882, 6753, 1988,    5,   37,    1,  133,\n           6,    4,   43,   80,    1,   10, 9999,   17]) tensor([[ 578,    6,    5,    2, 4012,    1],\n        [  36, 5533, 3473, 9999, 1370,    1],\n        [1231,   81,   15, 2255,    4, 2511],\n        [ 738, 9999,    0,    7,  141,    5],\n        [   7,   28,  202,  910,  506, 5503],\n        [  78,  184,   41,  737,    1, 2790],\n        [ 927,    6,  209,    5, 2738, 3297],\n        [9999,   75, 9999, 2551,   26,   13],\n        [ 205, 2494,    2,    3,   19, 2502],\n        [9999,    1,    0, 9999,   28,   18],\n        [  32,  383,    4, 1540,  421,    1],\n        [  93, 2592,    8, 1224, 9999,   26],\n        [   1,   44,  785,    0,  435,  475],\n        [9999,    0,  307,    6, 3359,    7],\n        [ 426,   41,  563,   60,  334, 9999],\n        [ 676,    2, 1494, 1345, 8215, 8216],\n        [1875, 2352,   56,   33,   44,  409],\n        [   7,   22,  337,  420,   52,  109],\n        [   1, 2167, 6752, 6754,    2, 6755],\n        [   0, 1381,   20,   27,   63, 1911],\n        [ 171,   10,   67,   90, 9999,    1],\n        [   2,  333,   28,  120,    3,  124],\n        [  14,    0, 1568,   11, 2832, 9999],\n        [   1, 9999,    6,    5, 2150, 1107],\n        [   0, 2056, 2357, 1529,    4, 2553],\n        [ 834, 9999,   41,  761,    1,    0],\n        [  18, 9999,    4, 9999,   60,   85],\n        [ 847,    7,   17,   78, 5634,   25],\n        [   8,    0,  177,   25, 2348,   25],\n        [3001,   44,  360,    0,  157,    7],\n        [   8,    0,  439,   30,  215,   42],\n        [   0, 1950, 9851, 1036,    9, 9852]]) tensor([[2253,    0,    4,  ...,   42,  441,  149],\n        [  18,  177, 4269,  ..., 7641,   68,   63],\n        [4349,   16,   62,  ...,  194,  362,   52],\n        ...,\n        [5176,   17, 1364,  ...,    7, 1239, 6742],\n        [3327, 8285,   53,  ...,   20, 2751,  791],\n        [  36, 9093,  307,  ...,   16, 3242,  944]])\n"
     ]
    }
   ],
   "source": [
    "for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        print(input_labels, pos_labels, neg_labels)\n",
    "        if i>5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss: 420.0462646484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 100, loss: 194.03431701660156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 200, loss: 174.1256561279297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 300, loss: 134.47421264648438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 400, loss: 185.26446533203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 500, loss: 120.10485076904297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 600, loss: 106.89739990234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 700, loss: 101.04762268066406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 800, loss: 52.14518356323242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 900, loss: 63.70637512207031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 1000, loss: 72.64402770996094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 1100, loss: 87.04966735839844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 1200, loss: 85.19691467285156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 1300, loss: 62.38524627685547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 1400, loss: 95.55689239501953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 1500, loss: 49.854061126708984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 1600, loss: 73.7475814819336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 1700, loss: 38.49934387207031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 1800, loss: 64.14765930175781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 1900, loss: 63.073734283447266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 2000, loss: 71.18134307861328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 2100, loss: 46.71556091308594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 2200, loss: 84.55611419677734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 2300, loss: 33.909854888916016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 2400, loss: 59.24190139770508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 2500, loss: 84.42159271240234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 2600, loss: 46.61289596557617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 2700, loss: 70.17655944824219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 2800, loss: 70.31050109863281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 2900, loss: 56.964500427246094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 3000, loss: 93.70215606689453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 0, loss: 35.208038330078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 100, loss: 33.77238082885742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 200, loss: 34.500885009765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 300, loss: 34.20140838623047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 400, loss: 33.3465461730957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 500, loss: 32.77040481567383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 600, loss: 33.08066940307617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 700, loss: 33.14141082763672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 800, loss: 33.91048812866211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 900, loss: 34.20799255371094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 1000, loss: 33.000858306884766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 1100, loss: 33.526451110839844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 1200, loss: 34.20500564575195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 1300, loss: 32.98984909057617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 1400, loss: 32.77361297607422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 1500, loss: 32.69524002075195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 1600, loss: 32.55683135986328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iter: 1700, loss: 32.62810516357422\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "#随机梯度下降\n",
    "\n",
    "for e in range(NUM_EPOCHS): #开始迭代\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        #print(input_labels, pos_labels, neg_labels)\n",
    "        \n",
    "        # TODO\n",
    "        input_labels = input_labels.long() #longtensor\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "        if USE_CUDA:\n",
    "            input_labels = input_labels.cuda()\n",
    "            pos_labels = pos_labels.cuda()\n",
    "            neg_labels = neg_labels.cuda()\n",
    "       \n",
    "        #下面第一节课都讲过的   \n",
    "        optimizer.zero_grad() #梯度归零\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        #打印结果。\n",
    "        if i % 100 == 0:\n",
    "            with open(LOG_FILE, \"a\") as fout:\n",
    "                fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(e, i, loss.item()))\n",
    "                print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
    "            \n",
    "        \n",
    "        # if i % 2000 == 0:\n",
    "        #     embedding_weights = model.input_embeddings()\n",
    "        #     sim_simlex = evaluate(\"simlex-999.txt\", embedding_weights)\n",
    "        #     sim_men = evaluate(\"men.txt\", embedding_weights)\n",
    "        #     sim_353 = evaluate(\"wordsim353.csv\", embedding_weights)\n",
    "        #     with open(LOG_FILE, \"a\") as fout:\n",
    "        #         print(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "        #             e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "        #         fout.write(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "        #             e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "                \n",
    "    embedding_weights = model.input_embeddings()\n",
    "    np.save(\"embedding-{}\".format(EMBEDDING_SIZE), embedding_weights)\n",
    "    torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"embedding-{}.th\".format(EMBEDDING_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 MEN 和 Simplex-999 数据集上做评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simlex-999 SpearmanrResult(correlation=0.17251697429101504, pvalue=7.863946056740345e-08)\n",
      "men SpearmanrResult(correlation=0.1778096817088841, pvalue=7.565661657312768e-20)\n",
      "wordsim353 SpearmanrResult(correlation=0.27153702278146635, pvalue=8.842165885381714e-07)\n"
     ]
    }
   ],
   "source": [
    "embedding_weights = model.input_embeddings()\n",
    "print(\"simlex-999\", evaluate(\"simlex-999.txt\", embedding_weights))\n",
    "print(\"men\", evaluate(\"men.txt\", embedding_weights))\n",
    "print(\"wordsim353\", evaluate(\"wordsim353.csv\", embedding_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 寻找nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good ['good', 'bad', 'perfect', 'hard', 'questions', 'alone', 'money', 'false', 'truth', 'experience']\n",
      "fresh ['fresh', 'grain', 'waste', 'cooling', 'lighter', 'dense', 'mild', 'sized', 'warm', 'steel']\n",
      "monster ['monster', 'giant', 'robot', 'hammer', 'clown', 'bull', 'demon', 'triangle', 'storyline', 'slogan']\n",
      "green ['green', 'blue', 'yellow', 'white', 'cross', 'orange', 'black', 'red', 'mountain', 'gold']\n",
      "like ['like', 'unlike', 'etc', 'whereas', 'animals', 'soft', 'amongst', 'similarly', 'bear', 'drink']\n",
      "america ['america', 'africa', 'korea', 'india', 'australia', 'turkey', 'pakistan', 'mexico', 'argentina', 'carolina']\n",
      "chicago ['chicago', 'boston', 'illinois', 'texas', 'london', 'indiana', 'massachusetts', 'florida', 'berkeley', 'michigan']\n",
      "work ['work', 'writing', 'job', 'marx', 'solo', 'label', 'recording', 'nietzsche', 'appearance', 'stage']\n",
      "computer ['computer', 'digital', 'electronic', 'audio', 'video', 'graphics', 'hardware', 'software', 'computers', 'program']\n",
      "language ['language', 'languages', 'alphabet', 'arabic', 'grammar', 'pronunciation', 'dialect', 'programming', 'chinese', 'spelling']\n"
     ]
    }
   ],
   "source": [
    "for word in [\"good\", \"fresh\", \"monster\", \"green\", \"like\", \"america\", \"chicago\", \"work\", \"computer\", \"language\"]:\n",
    "    print(word, find_nearest(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单词之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n",
      "henry\n",
      "charles\n",
      "pope\n",
      "queen\n",
      "iii\n",
      "prince\n",
      "elizabeth\n",
      "alexander\n",
      "constantine\n",
      "edward\n",
      "son\n",
      "iv\n",
      "louis\n",
      "emperor\n",
      "mary\n",
      "james\n",
      "joseph\n",
      "frederick\n",
      "francis\n"
     ]
    }
   ],
   "source": [
    "man_idx = word_to_idx[\"man\"] \n",
    "king_idx = word_to_idx[\"king\"] \n",
    "woman_idx = word_to_idx[\"woman\"]\n",
    "embedding = embedding_weights[woman_idx] - embedding_weights[man_idx] + embedding_weights[king_idx]\n",
    "cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "for i in cos_dis.argsort()[:20]:\n",
    "    print(idx_to_word[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}