{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word embedding\n",
    "\n",
    "使用skip-gram模型训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import os\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-85-2a4fed5f9180>, line 8)",
     "traceback": [
      "  File \"<ipython-input-85-2a4fed5f9180>\", line 8\n    EMBEDDING_SIZE = 100w\n                        ^\nSyntaxError: invalid syntax\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# params\n",
    "C = 3 # context window\n",
    "K = 100 # num of negative samples\n",
    "NUM_EPOCHS = 2\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.02\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xuming06/Codes/python-tutorial\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99111\n"
     ]
    }
   ],
   "source": [
    "with open('./data/nietzsche.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "text = text.split()\n",
    "print(len(text))\n",
    "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1))\n",
    "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
    "\n",
    "idx_to_word = {idx:word for idx,word in enumerate(vocab.keys())}\n",
    "word_to_idx = {word:i for i,word in idx_to_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'the'),\n (1, 'of'),\n (2, 'and'),\n (3, 'to'),\n (4, 'in'),\n (5, 'a'),\n (6, 'is'),\n (7, 'that'),\n (8, 'as'),\n (9, 'it')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(idx_to_word.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0),\n ('of', 1),\n ('and', 2),\n ('to', 3),\n ('in', 4),\n ('a', 5),\n ('is', 6),\n ('that', 7),\n ('as', 8),\n ('it', 9)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_idx.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = np.array([count for count in vocab.values()],dtype=np.float32)\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "word_freqs = word_freqs ** (3./4.)\n",
    "word_freqs = word_freqs / np.sum(word_freqs)\n",
    "VOCAB_SIZE = len(idx_to_word)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Dataset创建一个自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(data.Dataset):\n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):\n",
    "        super(WordEmbeddingDataset, self).__init__()\n",
    "        self.text_encoded = [word_to_idx.get(word, word_to_idx[\"<unk>\"]) for word in text]\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        center_word = self.text_encoded[index]\n",
    "        pos_indices = list(range(index - C)) + list(range(index + 1, index + C + 1))\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
    "\n",
    "        pos_words = self.text_encoded[pos_indices]  # 周围单词\n",
    "        neg_wrods = torch.multinomial(self.word_freqs, pos_words.shape[0], True)\n",
    "\n",
    "        return center_word, pos_words, neg_wrods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
    "\n",
    "dataloader = data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x12a754e48>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义pytorch模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        # input_label: [batch_size]\n",
    "        \n",
    "        input_embedding = self.in_embed(input_labels)\n",
    "        pos_embedding = self.in_embed(pos_labels)\n",
    "        neg_embedding = self.in_embed(neg_labels)\n",
    "        \n",
    "        input_embedding = input_embedding.unsqueeze(2)\n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding).squeeze()\n",
    "        neg_dot = torch.bmm(neg_embedding, input_embedding).squeeze()\n",
    "        \n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1)\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
    "        \n",
    "        loss = log_neg + log_pos\n",
    "        return -loss\n",
    "    \n",
    "    def input_embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/utils/data/_utils/collate.py\", line 79, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/utils/data/_utils/collate.py\", line 79, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 15324 and 98566 in dimension 1 at ../aten/src/TH/generic/THTensor.cpp:689\n",
     "traceback": [
      "",
      "RuntimeErrorTraceback (most recent call last)",
      "<ipython-input-101-0877973f23a1> in <module>()\n      1 optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n      2 for i in range(NUM_EPOCHS):\n----> 3     for j,(input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n      4         input_labels = input_labels.long()\n      5         pos_labels = pos_labels.long()\n",
      "/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/utils/data/dataloader.py in __next__(self)\n    817             else:\n    818                 del self._task_info[idx]\n--> 819                 return self._process_data(data)\n    820 \n    821     next = __next__  # Python 2 compatibility\n",
      "/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/utils/data/dataloader.py in _process_data(self, data)\n    844         self._try_put_index()\n    845         if isinstance(data, ExceptionWrapper):\n--> 846             data.reraise()\n    847         return data\n    848 \n",
      "/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/_utils.py in reraise(self)\n    383             # (https://bugs.python.org/issue2651), so we work around it.\n    384             msg = KeyErrorMessage(msg)\n--> 385         raise self.exc_type(msg)\n",
      "RuntimeError: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/utils/data/_utils/collate.py\", line 79, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/utils/data/_utils/collate.py\", line 79, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/Users/xuming06/Library/Python/3.6/lib/python/site-packages/torch/utils/data/_utils/collate.py\", line 55, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 15324 and 98566 in dimension 1 at ../aten/src/TH/generic/THTensor.cpp:689\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "for i in range(NUM_EPOCHS):\n",
    "    for j,(input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        input_labels = input_labels.long()\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print('epoch', i, 'iter', j, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}